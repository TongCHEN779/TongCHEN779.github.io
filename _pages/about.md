---
permalink: /
title: "Welcome to Tong CHEN's homepage"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a PhD student at LAAS-CNRS, POP team, in Toulouse, France, under supervision of Jean-Bernard Lasserre, Victor Magron and Edouard Pauwels since September 2019. I received a master's degree in September 2019 in Statistics and Machine Learning from Universit√© Paris Saclay in France, and a bachelor's degree in June 2017 in Mathematics from Wuhan University in China. Here is my CV.

My PhD thesis is about "Robustness Verification of Neural Networks using Polynomial Optimization", here are the slides for my PhD defense.

My research mainly focuses on optimization methods for and with machine learning. 

From optimization aspect, I have developed a second-order SDP relaxation method based on Lasserre's moment-SOS hierarchy, called Sublevel Hierarchy,  which can be used for robustness verification of deep neural networks. I am interested in exploring the sparse structure, together with designing efficient first-/second-order optimization algorithms for large-scale problems arising from machine learning. For example, the fast first-order SDP solver for robustness verification of DNNs by Dathathri et al 2020.

From machine learning aspect, I am concerned about the robustness and safety of intelligent systems. In particular, robustness verification of neural networks is a tough problem both theoretically and practically. There always exists a trade-off between efficiency and accuracy for verification algorithms. I have been working for a long time to design efficient polynomial optimization-based algorithms to verify robustness of neural networks, see Chen et al 2020, Chen et al 2021. However, polynomial-based approaches can be only applied to small-scaled networks for the moment. The potential and future of polynomial optimization in machine learning is still unknown.

Research Interests
======
* Polynomial optimization algorithms with/without exploring sparsity structures and their applications.
* Semidefinite programming (SDP) and first-order algorithms for SDP.
* Relaxation techniques for non-convex optimization, e.g., moment relaxation, sum-of-square method.
* First-/second-order optimization algorithms in machine learning.
* Safety and reliability of neural networks, e.g., robustness verification, robust training.
* Mathematical foundation of deep learning.

Research Projects
======
*
*
*