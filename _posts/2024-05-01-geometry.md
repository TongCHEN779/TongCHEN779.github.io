---
title: 'Differential Geometry and Deep Learning'
date: 2024-05-01
permalink: /posts/geo/
tags:
  - Riemannian Geometry
  - Deep Learning
  - Lie Group
  - Geodesic Distance
---

Background
---
In deep learning, models can be cast into three categories: 

1. Encoder-type models that take data to a latent space, and use simple linear regression to perform downstream task. Such models include neural networks for classification, regression, segmentation, etc.

2. Decoder-type models that parameterize data using low-dimensional latent vectors, and sample latent vectors from a prior simple distribution to generate data. Such models include AE, VAE, diffusion model, etc.

3. Adversarial-type models that trains both an encoder as a discriminator, and a decoder as a generator. Such models include GAN, WGAN, etc.

Note that training an encoder does not need a decoder, while training an decoder usually requires training an encoder simutaneously. See [[Schuster et al. 2021](https://arxiv.org/abs/2108.13910)] for training decoder without encoder.

It's usually assumed that data in high dimensional space are distributed near a low-dimensional manifold $\mathcal{M}$. An encoder acts like a local chart $(U, \varphi_U)$ of the hidden manifold, that maps an open set $U$ on the manifold to an open set $\varphi_U (U)$ in the latent space (a low-dimensional Euclidean space). And the corresponding decoder pulls the flattened openset $\varphi_U (U)$ in the latent space back to the manifold, through $\varphi_U^{-1}$. 

Riemannian Geometry
---

Learning Invariance
---

Generative Model
---