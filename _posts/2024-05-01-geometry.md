---
title: 'Differential Geometry and Deep Learning'
date: 2024-05-01
permalink: /posts/geo/
tags:
  - Riemannian Geometry
  - Deep Learning
  - Lie Group
  - Geodesic Distance
---

Background
---
In deep learning, models can be cast into three categories: 

1. Encoder-type models that take data to a latent space, and use simple linear regression to perform downstream task. Such models include neural networks for classification, regression, segmentation, etc.

2. Decoder-type models that parameterize data using low-dimensional latent vectors, and sample latent vectors from a prior simple distribution to generate data. Such models include AE, VAE, diffusion model, etc.

3. Adversarial-type models that trains both an encoder as a discriminator, and a decoder as a generator. Such models include GAN, WGAN, etc.

Note that training an encoder does not need a decoder, while training an decoder usually requires training an encoder simutaneously. See my previous [post](https://tongchen779.github.io/posts/gen/) about why we need an encoder to learn a decoder, also [[Schuster et al. 2021](https://arxiv.org/abs/2108.13910)] about how to train a decoder without an encoder..

It's usually assumed that data in high dimensional space are distributed near a low-dimensional manifold $\mathcal{M}$. An encoder acts like a local chart $(U, \varphi_U)$ of the hidden manifold, that maps an open set $U$ on the manifold to an open set $\varphi_U (U)$ in the latent space (a low-dimensional Euclidean space). And the corresponding decoder pulls the flattened openset $\varphi_U (U)$ in the latent space back to the manifold, through $\varphi_U^{-1}$. 

Riemannian Geometry
---
Briefly speaking, a **manifold** $\mathcal{M}$ is a set that is locally homeomorphic to Euclidean space $\mathbb{R}^m$. Here locally means each point $\mathbf{p} \in \mathcal{M}$ is associated with a **chart** $(U_{\mathbf{p}}, \mathbf{x}_{\mathbf{p}})$, such that $\mathbf{p} \in U_{\mathbf{p}}$ and $\mathbf{x}: U_{\mathbf{p}} \to \mathbf{x}_{\mathbf{p}} (U_{\mathbf{p}})$ is a homeomorphism. The collection of charts $\{(U_{\mathbf{p}}, \mathbf{x}_{\mathbf{p}})\}$ is called an **atlas**, and $\mathbf{x}_{\mathbf{p}}$ is called the local coordinates. An atlas on a manifold is called **differentiable** if all charts are coherent, i.e., the transitions

$$\mathbf{x}_{\mathbf{q}} \circ \mathbf{x}_{\mathbf{p}}^{-1}: \mathbf{x}_{\mathbf{p}} (U_{\mathbf{p}} \cap U_{\mathbf{q}}) \to \mathbf{x}_{\mathbf{q}} (U_{\mathbf{p}} \cap U_{\mathbf{q}})$$

are $C^{\infty}$-differentiable. As we see, each proper definition on a manifold is translated by its chart into an Euclidean space. For example, a function $f: \mathcal{M} \to \mathbb{R}$ with charts $(U_{\mathbf{p}}, \mathbf{x}_{\mathbf{p}})$ is called differentiable if all $f \circ \mathbf{x}_{\mathbf{p}}^{-1}$ are differentiable. A map $h: \mathcal{M} \to \mathcal{M}'$ with charts $(U_{\mathbf{p}}, \mathbf{x}_{\mathbf{p}})$ and $(U_{\mathbf{p}}', \mathbf{x}_{\mathbf{p}}')$ is called differentiable if all $\mathbf{x}_{\mathbf{q}}' \circ h \circ \mathbf{x}_{\mathbf{p}}^{-1}$ are differentiable.

Learning Invariance
---

Generative Model
---