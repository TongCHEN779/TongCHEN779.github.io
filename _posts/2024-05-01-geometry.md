---
title: 'Differential Geometry and Deep Learning'
date: 2024-05-01
permalink: /posts/geo/
tags:
  - Riemannian Geometry
  - Deep Learning
  - Lie Group
  - Geodesic Distance
---

Background
---
In deep learning, models can be cast into three categories: 

1. Encoder-type models that take data to a latent space, and use simple linear regression to perform downstream task. Such models include neural networks for classification, regression, segmentation, etc.

2. Decoder-type models that parameterize data using low-dimensional latent vectors, and sample latent vectors from a prior simple distribution to generate data. Such models include AE, VAE, diffusion model, etc.

3. Adversarial-type models that trains both an encoder as a discriminator, and a decoder as a generator. Such models include GAN, WGAN, etc.

Note that training an encoder does not need a decoder, while training an decoder usually requires training an encoder simutaneously. See my previous [post](https://tongchen779.github.io/posts/gen/) about why we need an encoder to learn a decoder, also [[Schuster et al. 2021](https://arxiv.org/abs/2108.13910)] about how to train a decoder without an encoder..

It's usually assumed that data in high dimensional space are distributed near a low-dimensional manifold $\mathcal{M}$. An encoder acts like a local chart $(U, \varphi_U)$ of the hidden manifold, that maps an open set $U$ on the manifold to an open set $\varphi_U (U)$ in the latent space (a low-dimensional Euclidean space). And the corresponding decoder pulls the flattened openset $\varphi_U (U)$ in the latent space back to the manifold, through $\varphi_U^{-1}$. 

Riemannian Geometry
---
Briefly speaking, a **manifold** $\mathcal{M}$ is a set that is locally homeomorphic to Euclidean space $\mathbb{R}^m$. Here locally means each point $p \in \mathcal{M}$ is associated with a **chart** $(U, \varphi_U)$, such that $p \in U$ and $\varphi_U: U \to \varphi_U (U)$ is a homeomorphism. The collection of charts $\{(U, \varphi_U)\}$ is called an **atlas**, and $\mathbf{x} = \varphi_U$ is called the local coordinates. An atlas on a manifold is called **differentiable** if all charts are coherent, i.e., the transitions

$$\varphi_V \circ \varphi_U^{-1}: \varphi_U (U \cap V) \to \varphi_V (U \cap V)$$

are $C^{\infty}$-differentiable. As we see, each proper definition on a manifold is translated through its local coordinates into an Euclidean space. For example, a function $f: \mathcal{M} \to \mathbb{R}$ with charts $(U, \varphi_U)$ is called differentiable if all $f \circ \varphi_U^{-1}$ are differentiable. A map $h: \mathcal{M} \to \mathcal{N}$ with charts $(U, \varphi_U)$ and $(V, \psi_V)$ is called differentiable if all $\psi_V \circ h \circ \varphi_U^{-1}$ are differentiable. 

Defining derivative operators on a manifold is much more complicated. At a hight level, for a given point $p \in \mathcal{M}$, the **tangent** space $T \mathcal{M}$ (of derivatives) and **cotangent** space $T^* \mathcal{M}$ (of differentials) is mutually dual space of finite dimension. Since the dual of dual is identity, we can either first define the tangent space and then the cotangent space, or the inverse. Whereas most of the textbooks start introducing the tangent space (because it's easier to illustrate in tangent space), we borrow the insight from [[Chern et al, 1999](https://books.google.dk/books?hl=en&lr=&id=Mvk7DQAAQBAJ)] to do the inverse, i.e., first define the cotangent space $T^* \mathcal{M}$ and then $T \mathcal{M}$ as the dual of $T^* \mathcal{M}$.

The space of $C^{\infty}$-differentiable functions at $p$, denoted by $C^{\infty}_p$, is an infinite-dimensional vector space. At an algebraic view, the space $C^{\infty}_p$ is not a concise object to study because there are any "redundant" elements that destroy the algebraic structure. For instance, we would like to regard two functions from $C^{\infty}_p$ as the same if they coincide with each other locally, since we only care about the local property at $p$. To this end, we introduce the quotient space $\mathcal{F}_p := C^{\infty}_p / \sim$, where the equivalent relation $\sim$ is defined by

$$f \sim g \Longleftrightarrow f\vert_U = g\vert_U \text{ for some } U.$$

The space $\mathcal{F}_p$ is still an infinite-dimensional vector space, but only contains the local information of functions at $p$. More interestingly, the vector space $\mathcal{F}_p$ is also a local ring, with unique maximal ideal $\mathfrak{m}_p := \lbrace [f]: f(p) = 0 \rbrace$ defined as the set of functions vanishing at $p$. Indeed, we can factorize $\mathcal{F}_p$ as $\mathbb{R} \oplus \mathfrak{m}_p$, hence $\mathcal{F}_p / \mathfrak{m}_p \cong \mathbb{R}$. Similarly, can further factorize $\mathcal{F}_p$ as $\mathbb{R} \oplus (\mathfrak{m}_p / \mathfrak{m}^2_p) \oplus \mathfrak{m}^2_p = \mathcal{H}_p \oplus (\mathfrak{m}_p / \mathfrak{m}^2_p)$, where $\mathcal{H}_p := \mathbb{R} \oplus \mathfrak{m}^2_p$. Now we arrive at the first definition of cotangent space:

$$T^* \mathcal{M} := \mathcal{F}_p / \mathcal{H}_p \cong \mathfrak{m}_p / \mathfrak{m}^2_p.$$

As we see, the ddiferential operator are defined as the canonical homomorphism $d_p: \mathcal{F}_p \to \mathcal{F}_p / \mathcal{H}_p$. The cotangent space is spanned by the natural basis $\lbrace d_p (x^i) \rbrace$, 

Learning Invariance
---

Generative Model
---