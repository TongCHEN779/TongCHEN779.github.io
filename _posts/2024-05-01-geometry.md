---
title: 'Differential Geometry and Deep Learning'
date: 2024-05-01
permalink: /posts/geo/
tags:
  - Riemannian Geometry
  - Deep Learning
  - Lie Group
  - Geodesic Distance
---

Background
---
In deep learning, models can be cast into three categories: 

1. Encoder-type models that take data to a latent space, and use simple linear regression to perform downstream task. Such models include neural networks for classification, regression, segmentation, etc.

2. Decoder-type models that parameterize data using low-dimensional latent vectors, and sample latent vectors from a prior simple distribution to generate data. Such models include AE, VAE, diffusion model, etc.

3. Adversarial-type models that trains both an encoder as a discriminator, and a decoder as a generator. Such models include GAN, WGAN, etc.

Note that training an encoder does not need a decoder, while training an decoder usually requires training an encoder simutaneously. See my previous [post](https://tongchen779.github.io/posts/gen/) about why we need an encoder to learn a decoder, also [[Schuster et al. 2021](https://arxiv.org/abs/2108.13910)] about how to train a decoder without an encoder..

It's usually assumed that data in high dimensional space are distributed near a low-dimensional manifold $\mathcal{M}$. An encoder acts like a local chart $(U, \varphi_U)$ of the hidden manifold, that maps an open set $U$ on the manifold to an open set $\varphi_U (U)$ in the latent space (a low-dimensional Euclidean space). And the corresponding decoder pulls the flattened openset $\varphi_U (U)$ in the latent space back to the manifold, through $\varphi_U^{-1}$. 

Riemannian Geometry
---
Briefly speaking, a **manifold** $\mathcal{M}$ is a set that is locally homeomorphic to Euclidean space $\mathbb{R}^m$. Here locally means each point $\mathbf{p} \in \mathcal{M}$ is associated with a **chart** $(U, \varphi_U)$, such that $\mathbf{p} \in U$ and $\varphi_U: U \to \varphi_U (U)$ is a homeomorphism. The collection of charts $\{(U, \varphi_U)\}$ is called an **atlas**, and $\mathbf{x} = \varphi_U$ is called the local coordinates. An atlas on a manifold is called **differentiable** if all charts are coherent, i.e., the transitions

$$\varphi_V \circ \varphi_U^{-1}: \varphi_U (U \cap V) \to \varphi_V (U \cap V)$$

are $C^{\infty}$-differentiable. As we see, each proper definition on a manifold is translated through its local coordinates into an Euclidean space. For example, a function $f: \mathcal{M} \to \mathbb{R}$ with charts $(U, \varphi_U)$ is called differentiable if all $f \circ \varphi_U^{-1}$ are differentiable. A map $h: \mathcal{M} \to \mathcal{N}$ with charts $(U, \varphi_U)$ and $(V, \psi_V)$ is called differentiable if all $\psi_V \circ h \circ \varphi_U^{-1}$ are differentiable. 

Defining derivative operators on a manifold is much more complicated. At a hight level, for a given point $\mathbf{p} \in \mathcal{M}$, the **tangent** space $T \mathcal{M}$ (of derivatives) and **cotangent** space $T^* \mathcal{M}$ (of differentials) is mutually dual space of finite dimension. Since the dual of dual is identity, we can either first define the tangent space and then the cotangent space, or the inverse. Whereas most of the textbooks start introducing the tangent space (because it's easier to illustrate in tangent space), we borrow the insight from [[Chern et al, 1999](https://books.google.dk/books?hl=en&lr=&id=Mvk7DQAAQBAJ)] to do the inverse, i.e., first define the cotangent space $T^* \mathcal{M}$ and then $T \mathcal{M}$ as the dual of $T^* \mathcal{M}$.

The space of $C^{\infty}$-differentiable functions at $\mathbf{p}$, denoted by $C^{\infty}_{\mathbf{p}}$, is an infinite-dimensional vector space. At an algebraic view, the space $C^{\infty}_{\mathbf{p}}$ is not a proper object to study because there are any "redundant" elements that destroy the algebraic structure. For instance, we would like to regard two functions from $C^{\infty}_{\mathbf{p}}$ as the same, if they coincide with each other locally. To this end, we introduce the quotient space $\mathcal{F}_{\mathbf{p}}$

Learning Invariance
---

Generative Model
---