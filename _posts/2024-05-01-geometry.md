---
title: 'Differential Geometry and Deep Learning'
date: 2024-05-01
permalink: /posts/geo/
tags:
  - Riemannian Geometry
  - Deep Learning
  - Lie Group
  - Geodesic Distance
---

Background
---
In deep learning, models can be cast into three categories: 

1. Encoder-type models that take data to a latent space, and use simple linear regression to perform downstream task. Such models include neural networks for classification, regression, segmentation, etc.

2. Decoder-type models that parameterize data using low-dimensional latent vectors, and sample latent vectors from a prior simple distribution to generate data. Such models include AE, VAE, diffusion model, etc.

3. Adversarial-type models that trains both an encoder as a discriminator, and a decoder as a generator. Such models include GAN, WGAN, etc.

Note that training an encoder does not need a decoder, while training an decoder usually requires training an encoder simutaneously. See my previous [post](https://tongchen779.github.io/posts/gen/) about why we need an encoder to learn a decoder, also [[Schuster et al. 2021](https://arxiv.org/abs/2108.13910)] about how to train a decoder without an encoder..

It's usually assumed that data in high dimensional space are distributed near a low-dimensional manifold $\mathcal{M}$. An encoder acts like a local chart $(U, \varphi_U)$ of the hidden manifold, that maps an open set $U$ on the manifold to an open set $\varphi_U (U)$ in the latent space (a low-dimensional Euclidean space). And the corresponding decoder pulls the flattened openset $\varphi_U (U)$ in the latent space back to the manifold, through $\varphi_U^{-1}$. 

Riemannian Geometry
---
Briefly speaking, a **manifold** $\mathcal{M}$ is a set that is locally homeomorphic to Euclidean space $\mathbb{R}^m$. Here locally means each point $p \in \mathcal{M}$ is associated with a **chart** $(U, \varphi_U)$, such that $p \in U$ and $\varphi_U: U \to \varphi_U (U)$ is a homeomorphism. The collection of charts $\{(U, \varphi_U)\}$ is called an **atlas**, and $\mathbf{x} = \varphi_U$ is called the local **coordinates**. An atlas on a manifold is called **differentiable** if all charts are consistent, i.e., the transitions

$$\varphi_V \circ \varphi_U^{-1}: \varphi_U (U \cap V) \to \varphi_V (U \cap V)$$

are $C^{\infty}$-differentiable. As we see, each proper definition on a manifold is translated through its local coordinates into an Euclidean space. For example, a function $f: \mathcal{M} \to \mathbb{R}$ with charts $(U, \varphi_U)$ is called differentiable if all $f \circ \varphi_U^{-1}$ are differentiable. A map $h: \mathcal{M} \to \mathcal{N}$ with charts $(U, \varphi_U)$ and $(V, \psi_V)$ is called differentiable if all $\psi_V \circ h \circ \varphi_U^{-1}$ are differentiable. 

Defining derivative operators on a manifold is much more complicated. At a hight level, for a given point $p \in \mathcal{M}$, the **tangent** space $T \mathcal{M}$ (of derivatives) and **cotangent** space $T^* \mathcal{M}$ (of differentials) is mutually dual space of finite dimension. Since the dual of dual is identity, we can either first define the tangent space and then the cotangent space, or the inverse. Whereas most of the textbooks start introducing the tangent space (because it's easier to illustrate in tangent space), we borrow the insight from [[Chern et al, 1999](https://books.google.dk/books?hl=en&lr=&id=Mvk7DQAAQBAJ)] to do the inverse, i.e., first define the cotangent space $T^* \mathcal{M}$ and then $T \mathcal{M}$ as the dual of $T^* \mathcal{M}$.

The space of $C^{\infty}$-differentiable functions at $p$, denoted by $C^{\infty}_p$, is an infinite-dimensional vector space. At an algebraic view, the space $C^{\infty}_p$ is not a concise object to study, because there are many "redundant" elements that destroy the algebraic structure. For instance, we would like to regard two functions from $C^{\infty}_p$ as the same if they coincide with each other locally, since we only care about the local property at $p$. To this end, we introduce the quotient space $\mathcal{F}_p := C^{\infty}_p / \sim$, where the equivalent relation $\sim$ is defined by

$$f \sim g \Longleftrightarrow f\vert_U = g\vert_U \text{ for some } U.$$

The space $\mathcal{F}_p$ is still an infinite-dimensional vector space, but only contains the local information of functions at $p$. The element in $\mathcal{F}_p$ is called a **germ**. More interestingly, the vector space $\mathcal{F}_p$ is also a local ring, with unique maximal ideal $\mathfrak{m}_p := \lbrace [f]: f(p) = 0 \rbrace$ defined as the set of functions vanishing at $p$. Indeed, we can factorize $\mathcal{F}_p$ as $\mathbb{R} \oplus \mathfrak{m}_p$, hence $\mathcal{F}_p / \mathfrak{m}_p \cong \mathbb{R}$. Similarly, can further factorize $\mathcal{F}_p$ as $\mathbb{R} \oplus (\mathfrak{m}_p / \mathfrak{m}^2_p) \oplus \mathfrak{m}^2_p = \mathcal{H}_p \oplus (\mathfrak{m}_p / \mathfrak{m}^2_p)$, where $\mathcal{H}_p := \mathbb{R} \oplus \mathfrak{m}^2_p$. Now we arrive at the first definition of cotangent space:

$$T^* \mathcal{M} := \mathcal{F}_p / \mathcal{H}_p \cong \mathfrak{m}_p / \mathfrak{m}^2_p.$$

As we see, the ddiferential operator are defined as the canonical homomorphism $\text{d}_p: \mathcal{F}_p \to \mathcal{F}_p / \mathcal{H}_p$. The cotangent space is spanned by the natural basis $\lbrace \text{d}_p (x^i) \rbrace$, i.e., differentials, and we have 

$$\text{d}_p ([f]) = \sum_{i = 1}^m \partial_i f(p) \text{d}_p (x^i), \text{ where } \partial_i f(p) := \frac{\partial (f \circ \varphi_U^{-1})}{\partial x^i} \bigg \vert_{x^i (p)}.$$

Here the way we define $\mathcal{H}_p$ is the linear space of germs of smooth functions whose partial derivatives with respect to local coordinates all vanish at $p$. We can also describe $\mathcal{H}_p$ through parameterized curves passing $p$. Denote by $\Gamma_p$ the set of $C^{\infty}$-curves

$$\gamma: (-\delta, \delta) \to \mathcal{M}, \; \gamma (0) = p.$$

Then the directional derivative of $[f] \in \mathcal{F}_p$ along $\gamma \in \Gamma_p$ at point $p$ is defined as a linear functional over $\mathcal{F}_p$:

$$\ll \gamma, \cdot \gg: \mathcal{F}_p \to \mathbb{R}, \; [f] \mapsto \frac{\text{d} (f \circ \gamma)}{\text{d} t} \big\vert_{t = 0}.$$

It's easy to show that the space $\mathcal{H}_p$ is exactly the set of those germs where all linear functionals $\ll \gamma, \cdot \gg$ vanish. This linear functional can be naturally extended to $T^* \mathcal{M}$ and linear with respect in the second variable. In order to make the form $\ll \cdot, \cdot \gg: \Gamma_p \times \mathcal{F}_p \to \mathbb{R}$ bilinear, we introduce an equivalent relation over $\Gamma_p$ as follows:

$$\gamma \sim \gamma' \Longleftrightarrow \ll \gamma, \text{d}_p ([f]) \gg = \ll \gamma', \text{d}_p ([f]) \gg.$$

We denote by $T \mathcal{M}$ the quotient space $\Gamma / \sim$, which is namely the tangent space and also the dual space of $T^* \mathcal{M}$ (actually this is not trivial and needs some justification). Now we have an induced bilinear form

$$\langle \cdot, \cdot \rangle: T \mathcal{M} \times T^* \mathcal{M} \to \mathbb{R}, \; ([\gamma], \text{d}_p ([f])) \mapsto \sum_{i = 1}^m a_i \xi^i,$$

where $a_i = \partial_i f(p)$ and $\xi^i = \frac{\text{d} x^i}{\text{d} t} \big \vert_{t = 0}$. The tangent space $T \mathcal{M}$ is spanned by partial derivative operators $\lbrace \partial_i \big \vert_p \rbrace$ on function germs, and we have

$$\big\langle \partial_k \big \vert_p, \text{d}_p (x^i) \big\rangle = \delta^i_k.$$

Generative Model
---
Now in the geometric view, an encoder is like a chart map $\varphi_U$ and a decoder is its inverse $\varphi_U^{-1}$. Learning an autoencoder type model is essentially learning the parameterization of a hidden manifold using a low-dimensional Euclidean latent space. Whether the manifold we learn is deterministic or statistic depends on whether we use reparameterization trick or not, the latter one is usually called a variational autoencoder (VAE). A diffusion model, or more general, a hierarchical autoencoder, is a "deep" version of autoencoder, that learns a complex manifold through the composition of simple manifolds. Note that in theory, it's not possible to parameterize a manifold globally using a single chart. For instance, there doesn't exist a diffeomorphism between the circle $\mathbb{S}^1$ and interval $[0,1]$. Fortunately, we don't have acces to all possible data on the hidden manifold, we only have a finite number of observations. Therefore, it is indeed possible to learn a diffeomorphism between a finite set $\mathcal{T} \subseteq \mathbb{S}^1$ and $\mathcal{Z} \subseteq [0,1]$. In this section, we are going to understand VAE and diffusion model through the lens of differential geometry.

### Variational Autoencoder [[Shao et al. 2017](https://arxiv.org/abs/1711.08014), [Arvanitidis et al. 2021](https://arxiv.org/abs/1710.11379), [Dai et al. 2021](https://arxiv.org/abs/2106.10777), [Chadebec et al. 2022](https://arxiv.org/abs/2209.07370)]
---


### Diffusion Model [[Park et al. 2023](https://arxiv.org/abs/2307.12868)]
---

### One Chart is NOT Enough [[Larsen et al. 2021](https://arxiv.org/abs/2102.00264), [Alberti et al. 2023](https://arxiv.org/abs/2303.15244)]
---