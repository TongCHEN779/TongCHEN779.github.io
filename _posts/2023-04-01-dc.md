---
title: 'Dataset Condensation: Theory, Practice, and Beyond'
date: 2024-04-01
permalink: /posts/dc/
tags:
  - Dataset Condensation
  - Integral Probability Metric
  - MMD
---

Background
---
Given a dataset $\mathcal{T} \subseteq \mathbb{R}^{n \times N}$ of size $N$ drawn from some unknown distribution $\mathcal{D}$, we would like to find a synthetic dataset $\mathcal{S} \subseteq \mathbb{R}^{n \times M}$ of size $M$, such that the gap between generalization loss of models trained over $\mathcal{T}$ and $\mathcal{S}$ is minimized:

$$\min_{\mathcal{S}} \; \vert L_{\mathcal{T}} (\theta_{\mathcal{S}}) - L_{\mathcal{T}} (\theta_{\mathcal{T}}) \vert,$$

where $\theta_{\mathcal{T}}$ is the minimizer of $L_{\mathcal{T}} (\theta)$, and $\theta_{\mathcal{S}}$ is the minimizer of $L_{\mathcal{S}} (\theta)$. In practice, we repalce everywhere the true generalization loss by the empirical estimation. Note that $L_{\mathcal{T}} (\theta_{\mathcal{S}}) \ge L_{\mathcal{T}} (\theta_{\mathcal{T}})$, and that $L_{\mathcal{T}} (\theta_{\mathcal{T}})$ is not related to $\mathcal{S}$. Hence it's equivalent to minimize $L_{\mathcal{T}} (\theta_{\mathcal{S}})$ over $\mathcal{S}$, and we have the following bi-level optimization problem:

$$\min_{\mathcal{S}} \; L_{\mathcal{T}} (\theta_{\mathcal{S}}), \; \text{s.t. } \theta_{\mathcal{S}} = \arg\min_{\theta} \; L_{\mathcal{S}} (\theta).$$

Performance-oriented Condensation
---
In this category, all methods aim to maximize the generalization property of the model trained over the synthetic dataset.

### Optimization-based

1. Dataset Distillation (Wang et al. 2018) [[link](https://arxiv.org/abs/1811.10959)]: Backpropagation-through-time (BPTT),

$$\begin{align*}
& \theta^{t+1}_{\mathcal{S}^t} \leftarrow \theta^t_{\mathcal{S}^t} - \eta \nabla_{\theta} L_{\mathcal{S}} (\theta^t_{\mathcal{S}^t}) \\
& \mathcal{S}^{t+1} \leftarrow \mathcal{S}^t - \alpha \nabla_{\mathcal{S}} L_{\mathcal{T}} (\theta^{t+1}_{\mathcal{S}^t})
\end{align*}$$

3. Dataset Distillation with Convexified Implicit Gradients (Loo et al. 2023) [[link](https://arxiv.org/abs/2302.06755)]: Implicit differentiation,

$$\begin{align*}
& \mathcal{S}^{t+1} \leftarrow \mathcal{S}^t - \alpha \frac{\text{d} L_{\mathcal{T}}}{\text{d} \mathcal{S}} \\
& \frac{\text{d} L_{\mathcal{T}}}{\text{d} \mathcal{S}} = \frac{\partial L_{\mathcal{T}} (\theta_{\mathcal{S}})}{\partial \mathcal{S}} + \frac{\partial}{\partial \mathcal{S}} \bigg(\frac{\partial L_{\mathcal{S}} (\theta)}{\partial \theta}^T v\bigg), v = \bigg(\frac{\partial^2 L_{\mathcal{S}} (\theta)}{\partial \theta \partial \theta^T}\bigg)^{-1} \frac{\partial L_{\mathcal{T}} (\theta_{\mathcal{S}})}{\partial \theta}
\end{align*}$$

### Gradient-based
---

1. Dataset Condensation with Gradient Matching (Zhao et al. 2020) [[link](https://arxiv.org/abs/2006.05929)]

$$\min_{\mathcal{S}} \max_{\theta} \; \mathbb{E}_{\tau \sim SGD} [d (\nabla_{\theta} L_{\mathcal{S}} (\theta), \nabla_{\theta} L_{\mathcal{T}} (\theta))]$$

2. Loss-Curvature Matching for Dataset Selection and Condensation (Shin et al. 2023) [[link](https://arxiv.org/abs/2303.04449)]

$$\min_{\mathcal{S}} \bigg( \max_{\theta} \; \underbrace{\mathbb{E}_{\tau \sim SGD} [d (\nabla_{\theta} L_{\mathcal{S}} (\theta), \nabla_{\theta} L_{\mathcal{T}} (\theta))]}_{\text{gradient matching}} + \underbrace{\mathcal{R} (\varepsilon, \theta, \lambda_{\max}^{\mathcal{T}, \mathcal{S}})}_{\text{curvature regularization}} \bigg)$$

### Parameter-based
---

1. Dataset Distillation by Matching Training Trajectories (Cazenavette et al. 2022) [[link](https://arxiv.org/abs/2203.11932)]

$$\min_{\mathcal{S}} \max_{\theta} \; \mathbb{E}_{\tau \sim SGD} [d (\theta_{\mathcal{S}}, \theta_{\mathcal{T}})]$$

### Feature-based
---

1. Dataset Distillation with Infinitely Wide Convolutional Networks (Nguyen et al. 2021) [[link](https://arxiv.org/abs/2107.13034)]

$$\min_{\mathcal{S}} \; d(\mathbf{y}_{\mathcal{T}}, \theta_{\mathcal{S}} (\mathbf{X}_{\mathcal{T}})), \; \theta_{\mathcal{S}} = k(\cdot, \mathbf{X}_{\mathcal{S}}) \big(k(\mathbf{X}_{\mathcal{S}}, \mathbf{X}_{\mathcal{S}}) + \lambda \mathbf{I}\big)^{-1} \mathbf{y}_{\mathcal{S}}, \; k = NTK$$

2. Dataset Meat-Learning from Kernel Ridge-Regression (Nguyen et al. 2020) [[link](https://arxiv.org/abs/2011.00050)]

$$\min_{\mathcal{S}} \; d(\mathbf{y}_{\mathcal{T}}, \theta_{\mathcal{S}} (\mathbf{X}_{\mathcal{T}})), \; \theta_{\mathcal{S}} = k(\cdot, \mathbf{X}_{\mathcal{S}}) \big(k(\mathbf{X}_{\mathcal{S}}, \mathbf{X}_{\mathcal{S}}) + \lambda \mathbf{I}\big)^{-1} \mathbf{y}_{\mathcal{S}}, \; k = NTK$$

3. CAFE: Learning to Condense Dataset by Aligning Features (Wang et al. 2022) [[link](https://arxiv.org/abs/2203.01531)]

$$\begin{align*}
\min_{\mathcal{S}} \; \bigg(\underbrace{\mathbb{E}_{\tau} \big[d(\theta_{\mathcal{S}} (\mathbf{X}_{\mathcal{S}}), \theta_{\mathcal{T}} (\mathbf{X}_{\mathcal{T}}))\big]}_{\text{intermediate feature matching}} + \underbrace{d(\mathbf{y}_{\mathcal{T}}, \theta_{\mathcal{S}} (\mathbf{X}_{\mathcal{T}}))}_{\text{output matching}} + \underbrace{L_{\mathcal{T}} (\theta_{\mathcal{S}})}_{\text{loss matching}}\bigg)
\end{align*}$$

Distribution-oriented Condensation
---
The main issue with performance-oriented condensation methods is, the synthetic dataset is very likely to overfit to downstream performance. However, if we are using performance as an aspect to match two probability distributions, the synthetic dataset might follow a totally different distribution while preserve the same downstream performance.

The core of distribution-oriented condensation methods is using a metric to measure the distance between two distributions: 

$$\min_{\mathcal{S}} \; d(\mu_{\mathcal{S}}, \mu_{\mathcal{T}}),$$

where $\mu_{\mathcal{T}}$ is the empirical distribution of $\mathcal{T}$ defined as $\mu_{\mathcal{T}} = \frac{1}{N} \sum_{i = 1}^N \delta_{\mathbf{x}_i}.$

1. Dataset Condensation with Distribution Matching (Zhao et al. 2021) [[link](https://arxiv.org/abs/2110.04181)]

$$IPM (\mu, \nu) = \max_{\theta} \; \big\vert\mathbb{E}_{X \sim \mu} [\theta(X)] - \mathbb{E}_{Y \sim \nu} [\theta(Y)]\big\vert$$

2. M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy (Zhang et al. 2023) [[link](https://arxiv.org/abs/2312.15927)]

$$\begin{align*}
MMD (\mu, \nu) & = \max_{\vert\vert \theta \vert\vert_{\mathcal{H}} \le 1} \big\vert\mathbb{E}_{X \sim \mu} [\theta(X)] - \mathbb{E}_{Y \sim \nu} [\theta(Y)]\big\vert^2 \\
& = \mathbb{E}_{X \sim \mu, X' \sim \mu} [k (X, X')] - 2 \mathbb{E}_{X \sim \mu, Y \sim \nu} [k (X, Y)] + \mathbb{E}_{Y \sim \nu, Y' \sim \nu} [k (Y, Y')]
\end{align*}$$

3. Dataset Condensation by Minimizing Wasserstein Distance (Chen et al. 2024) [[link]()]

$$\begin{align*}
W_1 (\mu, \nu) & = \max_{\vert\vert \theta \vert\vert_{Lip} \le 1} \big\vert\mathbb{E}_{X \sim \mu} [\theta(X)] - \mathbb{E}_{Y \sim \nu} [\theta(Y)]\big\vert \\
& = \min_{\gamma \in \Gamma (\mu, \nu)} \bigg\{\mathbb{E}_{(X, Y) \sim \gamma} [d(X, Y)]: \int \gamma(\cdot, y) \text{d} y = \mu, \; \int \gamma(x, \cdot) \text{d} x = \nu \bigg\}
\end{align*}$$

Integral Probability Metric (IPM): A Unified View
---

Variants
---

### Data Augmentation
---

1. Dataset Condensation with Differentiable Siamese Augmentation (Zhao et al. 2021) [[link](https://arxiv.org/abs/2102.08259)]: Gradient matching + data augmentation,

$$\mathcal{A}_{\omega}: \mathbb{R}^n \longrightarrow \mathbb{R}^n, \; \mathbf{x} \mapsto \mathcal{A}_{\omega} (\mathbf{x})$$

2. Dataset Condensation via Efficient Synthetic-Data Parameterization (Kim et al. 2022) [[link](https://arxiv.org/abs/2205.14959)]: Gradient matching + multi-formation + data augmentation,

$$f_c: \mathbb{R}^n \longrightarrow \mathbb{R}^{n'}, \; \mathbf{x} \mapsto f(\mathbf{x})$$

3. Dataset Distillation with Channel Efficient Process (Zhou et al. 2024) [[link](https://ieeexplore.ieee.org/abstract/document/10446099)]: Gradient matching + channel-wise multi-formation,

$$f_c: \mathbb{R}^n \longrightarrow \mathbb{R}^{4n}, \; \mathbf{x} \mapsto f_c(\mathbf{x})$$

### Space Changement
---

#### Data space

1. Synthesizing Informative Training Samples with GAN (Zhao et al. 2022) [[link](https://arxiv.org/abs/2204.07513)]: Distribution matching + pre-trained GAN + data augmentation,

$$IPM (\mu, \nu) = \max_{\theta} \; \big\vert\mathbb{E}_{G(Z) \sim \mu} [\theta(G(Z))] - \mathbb{E}_{Y \sim \nu} [\theta(Y)]\big\vert$$

2. Dataset Condensation via Generative Model (Zhang et al. 2023) [[link](https://arxiv.org/abs/2309.07698)]: Feature matching + GAN training + regularization,

$$\min_{Z_{\mathcal{S}}} \; \bigg(\underbrace{\mathbb{E}_{\tau} \big[d(\theta_{\mathcal{S}} (G(\mathbf{Z}_{\mathcal{S}})), \theta_{\mathcal{T}} (\mathbf{X}_{\mathcal{T}}))\big]}_{\text{feature matching}} + \underbrace{L (G)}_{\text{GAN loss}} + \underbrace{\mathcal{R}_{intra}}_{\text{intra-class diversity}} + \underbrace{\mathcal{R}_{inter}}_{\text{inter-class discrimination}}\bigg)$$

3. DiM: Distilling Dataset into Generative Model (Wang et al. 2023) [[link](https://arxiv.org/abs/2303.04707)]: Distribution matching + GAN training,

$$\min_{Z} \; \bigg(\underbrace{\mathbb{E}_{\theta \sim \mathcal{M}} \big[\big\vert\mathbb{E}_{G(Z) \sim \mu} [\theta(G(Z))] - \mathbb{E}_{Y \sim \nu} [\theta(Y)]\big\vert\big]}_{\text{logit matching}} + \underbrace{L (G)}_{\text{GAN loss}}\bigg)$$

4. Generalizing Dataset Distillation via Deep Generative Prior (Cazenavette et al. 2023) [[link](https://arxiv.org/abs/2305.01649)]: DC + pre-trained GAN,

$$\min_{\mathcal{Z}_{\mathcal{S}}} \; L_{DC} (G(\mathcal{Z}_{\mathcal{S}}), \mathcal{T})$$

5. Efficient Dataset Distillation via Minimax Diffusion (Gu et al. 2024) [[link](https://arxiv.org/abs/2311.15529)]: Feature matching + diffusion training + regularization,

$$\min_{Z_{\mathcal{S}}} \; \bigg(\underbrace{\mathbb{E}_{\tau} \big[d(Z_{\mathcal{S}}, Z_{\mathcal{T}})\big]}_{\text{feature (data) matching}} + \underbrace{L (E)}_{\text{diffusion loss}} + \underbrace{\mathcal{R}_{intra}}_{\text{intra-class diversity}}\bigg)$$

6. Dataset Distillation via Factorization (Liu et al. 2022) [[link](https://arxiv.org/abs/2210.16774)]: DC + hallucinator-extractor training,

$$\min_{\mathcal{Z}_{\mathcal{S}}} \; L_{DC} (F(\mathcal{Z}_{\mathcal{S}}), \mathcal{T}) + \underbrace{L_{sim} (F)}_{\text{similarity loss}}, \; \min_{F} \; \underbrace{L_{con} (F)}_{\text{contrasive loss}} + \underbrace{L_{task} (F)}_{\text{performance loss}}$$

#### Model space

1. Dataset Condensation via Expert Subspace Projection (Ma et al. 2023) [[link](https://www.mdpi.com/1424-8220/23/19/8148#:~:text=The%20condensation%20process%20utilizes%20a,stage%20updates%20the%20synthetic%20dataset.)]: DC + parameter span,

$$\begin{align*}
& \theta^{t+1}_{\mathcal{S}^t} \leftarrow \mathcal{P}_{\mathbb{S}_{\tau}} \big(\theta^t_{\mathcal{S}^t} - \eta \nabla_{\theta} L_{\mathcal{S}} (\theta^t_{\mathcal{S}^t})\big) \\
& \mathcal{S}^{t+1} \leftarrow \mathcal{S}^t - \alpha \nabla_{\mathcal{S}} L_{\mathcal{T}} (\theta^{t+1}_{\mathcal{S}^t})
\end{align*}$$

2. Efficient Dataset Distillation using Random Feature Approximation (Loo et al. 2022) [[link](https://arxiv.org/abs/2210.12067)]: Replace NTK by Neural Network Gaussian Process (NNGP),

$$\min_{\mathcal{S}} \; d(\mathbf{y}_{\mathcal{T}}, \theta_{\mathcal{S}} (\mathbf{X}_{\mathcal{T}})), \; \theta_{\mathcal{S}} = k(\cdot, \mathbf{X}_{\mathcal{S}}) \big(k(\mathbf{X}_{\mathcal{S}}, \mathbf{X}_{\mathcal{S}}) + \lambda \mathbf{I}\big)^{-1} \mathbf{y}_{\mathcal{S}}, \; k = NNGP$$

### Loss Improvement
---

1. Dataset Condensation with Contrastive Signals (Lee et al. 2022) [[link](https://arxiv.org/abs/2202.02916)]

2. Improved Distribution Matching for Dataset Condensation (Zhao et al. 2023) [[link](https://arxiv.org/abs/2307.09742)]

3. DataDAM: Efficient Dataset Distillation with Attention Matching (Sajedi et al. 2023) [[link](https://arxiv.org/abs/2310.00093)]

### Efficiency Improvement

1. DREAM: Efficient Dataset Distillation by Representative Matching (Liu et al. 2023) [[llink](https://arxiv.org/abs/2302.14416)]