---
title: 'Dataset Condensation: Theory, Practice, and Beyond'
date: 2024-04-01
permalink: /posts/dc/
tags:
  - Dataset Condensation
  - Integral Probability Metric
  - MMD
---

Background
---

Performance-oriented Condensation
---

### Optimization-based

1. Dataset Distillation

### Gradient-based
---

1. Dataset Condensation with Gradient Matching

1. Loss-Curvature Matching for Dataset Selection and Condensation

1. Dataset Distillation with Convexified Implicit Gradients

### Parameter-based
---

1. Dataset Distillation by Matching Training Trajectories

### Kernel-based
---

1. Dataset Distillation with Infinitely Wide Convolutional Networks

1. Dataset Meat-Learning from Kernel Ridge-Regression

### Feature-based
---

1. CAFE: Learning to Condense Dataset by Aligning Features

Distribution-oriented Condensation
---

1. Dataset Condensation with Distribution Matching

1. M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy

Integral Probability Metric (IPM): A Unified View
---

Variants
---

### Data Augmentation
---

1. Dataset Condensation with Differentiable Siamese Augmentation

1. Dataset Condensation via Efficient Synthetic-Data Parameterization

####

### Space Changement
---

#### Data space

1. Synthesizing Informative Training Samples with GAN

1. Dataset Condensation via Generative Model

1. Dataset Distillation with Channel Efficient Process

1. Dataset Distillation via Factorization

1. DiM: Distilling Dataset into Generative Model

1. Generalizing Dataset Distillation via Deep Generative Prior

#### Model space

1. Dataset Condensation via Expert Subspace Projection

### Loss Improvement
---

1. Dataset Condensation with Contrastive Signals

1. Improved Distribution Matching for Dataset Condensation

1. DataDAM: Efficient Dataset Distillation with Attention Matching

### Efficiency Improvement

1. DREAM: Efficient Dataset Distillation by Representative Matching