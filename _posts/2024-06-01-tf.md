---
title: 'From Transformers to Mamba: What Should We Pay Attention To?'
date: 2024-06-01
permalink: /posts/tf/
tags:
  - Graph Neural Network
  - Permutation Invariance
  - Node Representation
  - Edge Representation
---

Learning Sequential Inputs
---

Recurrent Neural Network (RNN)
---

Long Short-Term Memory (LSTM)
---

Transformers: Attention is All You Need
---

### Low-Rank Adaptation (LoRA)

### Vision Transformers (ViT)

State Space Model (SSM): Maybe Attention isn't All You Need
---